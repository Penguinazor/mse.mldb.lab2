{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection lab - WDBC\n",
    "Lab developed by Gary Marigliano - 07.2018\n",
    "\n",
    "Now that you are a little bit more familiar with the feature selection, you are going to compare multiple features selection techniques on a real-life dataset. The Breast Cancer Wisconsin (Diagnostic) Data Set (WDBC) is a dataset that contains 30 features (computed from digitalized images). You can have the full details [here](https://www.kaggle.com/uciml/breast-cancer-wisconsin-data).\n",
    "\n",
    "The 1st column is the sample id, the 2nd contains the class (either \"M\" or \"B\" respectively malignant and benign) and the 30 last columns the features as real numbers.\n",
    "\n",
    "## Lab goals\n",
    "\n",
    "* Discover, use and compare some features selection algorithms with a real-life dataset\n",
    "* Assess the quality of the selected features given by the algorithms\n",
    "\n",
    "\n",
    "## TODO in this notebook\n",
    "\n",
    "* Answer the questions in this notebook (where **TODO student** is written)\n",
    "* Take a look at the [skfeature](http://featureselection.asu.edu/) python library. You can/should use some features selection algorithms listed here (the python library has already been installed for this project): http://featureselection.asu.edu/html/skfeature.function.html and http://featureselection.asu.edu/tutorial.php"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "            ...             radius_worst  texture_worst  perimeter_worst  \\\n",
       "0           ...                    25.38          17.33           184.60   \n",
       "1           ...                    24.99          23.41           158.80   \n",
       "2           ...                    23.57          25.53           152.50   \n",
       "3           ...                    14.91          26.50            98.87   \n",
       "4           ...                    22.54          16.67           152.20   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn import preprocessing\n",
    "import scipy\n",
    "\n",
    "filename = r\"datasets/WDBC/data_WDBC.csv\"\n",
    "df = pd.read_csv(filename, sep=\",\")\n",
    "df = df.dropna(axis=1) # remove last colunm which only contains NaN values\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df.drop(['id', 'diagnosis'], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classes are: ['B' 'M']\n",
      "X contains (n_samples, n_features) = (569, 30)\n"
     ]
    }
   ],
   "source": [
    "# transform the categorical diagnosis values into numerical values. This is required by many algorithms.\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df[\"diagnosis\"].values)\n",
    "print(\"The classes are:\",le.classes_)\n",
    "y = le.transform(df[\"diagnosis\"].values)\n",
    "\n",
    "print(\"X contains (n_samples, n_features) =\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract good features using features selection algorithms\n",
    "\n",
    "In this part you will extract good features by trying several features selection algorithms. \n",
    "\n",
    "Remember that to extract relevant features you can use filter, wrapper or embedded based methods. You can use machine learning based techniques such as read the synaptic weights of an ANN, get the features importance from an ExtraTree,... or you can use statistical based techniques such as analyzing the variance of the variables.\n",
    "\n",
    "\n",
    "**TODO student**\n",
    "\n",
    "* Pick 3 features selection techniques from http://featureselection.asu.edu/html/skfeature.function.html\n",
    "* For each of them:\n",
    "    * Explain using 20-40 words the idea behind this feature selection technique. You will need to do some research to do that. If you don't understand it, pick an other ;-)\n",
    "    * Indicate the family of this feature selection technique (filter, wrapper, embedded, statistical/ML-based,...)\n",
    "    * Plot the features importance (the same way you did in the previous notebook)\n",
    "    * Choose N features to you find relevant\n",
    "    * Justify N and the chosen features\n",
    "    * Analyze the stability of the selected features (does the FS algorithm always return the same list of features ? Prove it)\n",
    "* Comment the 3 features importance plots. Here are some clues about the questions you should ask yourself:\n",
    "    * Are the features selected by the 3 FS techniques similars ?\n",
    "    * Are the number of features selected by the 3 FS techniques the same ?\n",
    "* Keep the lists of selected features. You will need them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO student..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess the selected features\n",
    "\n",
    "Now that you have lists of features (i.e. the 3 lists of N features your chosen FS techniques gave you), we are going to assess the relevance of these features.\n",
    "\n",
    "To do that you are going to create a function that takes a list of features as input and returns one or more score metrics (accuracy, f1-score, sensibility, specificity,...) for this given list. Inside that function several classifiers will be used to evaluate the performance they can achieve using the selected features. Here is an example of this function:\n",
    "\n",
    "``` python\n",
    "def evaluate_features(features):\n",
    "    score_clf_1 = compute_score_using_classifier_1(features)\n",
    "    score_clf_2 = compute_score_using_classifier_2(features)\n",
    "    score_clf_3 = compute_score_using_classifier_3(features)\n",
    "    # ....\n",
    "    return find_a_way_to_show_these_scores_nicely(score_clf_1, score_clf_2, score_clf_3)\n",
    "```\n",
    "\n",
    "You may be careful to the following points:\n",
    "* the classifiers you use may not be determinist therefore you may want to run them multiple time to have an averaged score\n",
    "* try to choose classifiers that are relatively different regarding how they use the data. Using 3 classifiers that are tree-based is not the best idea you can have\n",
    "* try to choose classifiers that you didn't use to get the lists of features in the first place\n",
    "\n",
    "**TODO student**\n",
    "\n",
    "* Write the `evaluate_features()` function with at least 3 classifiers (for example ANN, SVM and KNN)\n",
    "* Use this function with the lists you got from your FS algorithms\n",
    "* Use this function with a random list of selected features (same size as the others lists)\n",
    "* Use this function with all the features\n",
    "* Make a plot similar to the one just below (see https://matplotlib.org/examples/api/barchart_demo.html)\n",
    "* Comment the results. Here are some clues about the questions you should ask yourself:\n",
    "    * How the scores of the lists of selected features behave compare to the random/all features ?\n",
    "    * How behave the classifiers inside `evaluate_features()` ? Do they prefer a list in general ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/02-WDBC-perf-plot.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO student..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going further (optional)\n",
    "\n",
    "Now that you finished this notebook, it can be interesting to go a step further and try the points below:\n",
    "\n",
    "* Can we have better results (i.e. more relevant features and/or less features) if the input data are normalized ?\n",
    "* Compare the execution time between the FS algorithms you used. Given this additional information do you think you can exclude or prefer some FS techniques *for this particular case* ?\n",
    "* Plot the classifier performance for the K best features where K is $1, 2,..,k_{-1},k$ and comment the results\n",
    "* Anything you find relevant...\n",
    "\n",
    "Please answer to these questions just below in this same notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
