{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection lab - Noisy Iris\n",
    "Lab developed by Gary Marigliano - 07.2018\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, the [famous iris](https://en.wikipedia.org/wiki/Iris_flower_data_set) dataset has been modified to insert noisy features. The goal is to retrieve the 4 original features (sepal length/width and petal length/width) using features selection techniques.\n",
    "\n",
    "You can use some features selection algorithms listed here (the python library should already been installed for this project): http://featureselection.asu.edu/html/skfeature.function.html and http://featureselection.asu.edu/tutorial.php\n",
    "\n",
    "## TODO in this notebook\n",
    "\n",
    "Answer the questions in this notebook (where **TODO student** is written)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the dataset is modified to create new noisy features to the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5.1          3.5          1.4          0.2         12.           2.70309431\n",
      "    3.940532     3.82192499   2.06941402   4.76702567   2.06560061\n",
      "    6.2450302    5.85849349   5.78792798   1.05536523   5.88342003\n",
      "    1.13259417   3.17227085   0.33506431  -1.49546013   0.8143177\n",
      "    1.40862615]\n",
      " [  4.9          3.           1.4          0.2         12.           2.20309431\n",
      "    3.440532     3.32192499   1.56941402   4.26702567   1.56560061\n",
      "    5.7450302    5.35849349   5.28792798   0.55536523   5.38342003\n",
      "    0.63259417   2.67227085  -0.16493569  -1.99546013   0.3143177\n",
      "    0.90862615]\n",
      " [  4.7          3.2          1.3          0.2         12.           2.40309431\n",
      "    3.640532     3.52192499   1.76941402   4.46702567   1.76560061\n",
      "    5.9450302    5.55849349   5.48792798   0.75536523   5.58342003\n",
      "    0.83259417   2.87227085   0.03506431  -1.79546013   0.5143177\n",
      "    1.10862615]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "## Add some noisy features in the iris dataset\n",
    "\n",
    "# Add a feature that is always equals to a constant independently of the output --> useless feature\n",
    "constant_features = np.array([[12 for _ in range(X.shape[0])]]).transpose()\n",
    "X = np.append(X, constant_features, axis=1)\n",
    "\n",
    "# Add random noisy features. \n",
    "# These features are created using the first feature values with a more or less important noise level\n",
    "noise_levels = np.arange(1, 6, 0.3)\n",
    "first_feat = X[:, 1]\n",
    "\n",
    "n_samples = X.shape[0]\n",
    "for k in noise_levels:\n",
    "    noise = k*(np.random.rand() * 2 - 1)\n",
    "    noisy_features = [noise + first_feat[x] for x in range(n_samples)]\n",
    "    noisy_features = np.array([noisy_features]).transpose()\n",
    "    X = np.append(X, noisy_features, axis=1)\n",
    "\n",
    "# Here we can see that the 5th column is always equals to 12. The colunms after it are the noisy features.\n",
    "print(X[:3, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we start selecting relevant features. To do that, we prepare the data to be \"machine learning ready\".\n",
    "\n",
    "This means that we need to split the data into a train set and a test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this first notebook, the example below shows how to train and get the features sorted by decreasing importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 0.980\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# train\n",
    "clf = ExtraTreesClassifier(n_jobs=2, n_estimators=10)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# get the score\n",
    "score = clf.score(X_test, y_test)\n",
    "print(\"score {:.3f}\".format(score))\n",
    "\n",
    "# rank the features\n",
    "importances = clf.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "# get the features sorted by decreasing importance\n",
    "feat_importances_sorted = [(indices[f], importances[indices[f]]) for f in range(n_features)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO student**: \n",
    "* Plot the features importances with a bar chart (see picture below)\n",
    "* Comment the plot you just made. Here are some clues about the questions you should ask yourself:\n",
    "   * What this plot represents ?\n",
    "   * How do you compare two features using this graph ?\n",
    "   * How would you choose a \"good\" number of good features ?\n",
    "   * How can you be sure that the features you select are relevant ? What kind of tasks should you do ?\n",
    "   * How could you prove it ?\n",
    "   * For this modified dataset, is it really useful to reduce the number of features ?\n",
    "   * How easy/hard it is to retrieve the original features ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/01-noisy-iris-feat-importances-example.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO student..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing the relevance of the features' score\n",
    "\n",
    "Now that we have the features sorted by decreasing importances, your task is to choose which ones to keep.\n",
    "\n",
    "**TODO student**:\n",
    "\n",
    "* Choose N features that you find relevant\n",
    "* Justify the N number you chose\n",
    "* Compare, using a confusion matrix or an other relevant score metric, the classifier performance between:\n",
    "    * your selected features and the noisy iris dataset \n",
    "    * your selected features and some N random features (take the average score of K runs for the random features)\n",
    "    * your selected features and the worse N features (look at your feature importance plot)\n",
    "* Comment the obtained results. Here are some clues about the questions you should ask yourself:\n",
    "    * Among the features you select how many are the original features ?\n",
    "    * Is the useless feature (the one that always contains the same value) selected ?\n",
    "    \n",
    "To plot a prettier confusion matrix you can use this class:\n",
    "\n",
    "``` python\n",
    "from ConfusionMatrix import ConfusionMatrix\n",
    "y_pred = clf.predict(X_test_random)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "n_classes = len(np.unique(y))\n",
    "ConfusionMatrix.plot(cm, classes=range(n_classes), title=\"Confusion Matrix\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO student..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going further (optional)\n",
    "\n",
    "Now that you finished this notebook, it can be interesting to go a step further and try the points below:\n",
    "\n",
    "* Can we have better results (i.e. more relevant features and/or less features) if the input data are normalized ?\n",
    "* Can we retrieve the same relevant features with another features selection technique ?\n",
    "* Are the selected features only relevant for this specific classifier or do they generalize well with other classifiers ?\n",
    "* Plot the classifier performance for the K best features where K is $1, 2,..,k_{-1},k$ and comment the results\n",
    "* Anything you find relevant...\n",
    "\n",
    "Please answer to these questions just below in this same notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
