{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 02 - Model selection - Exploration of TREFLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab developed by: Diogo  Leite - 03.2019<br>\n",
    "Trefle algorithm: Gary Marigliano. (Based on the PhD thesis of Carlos Pe√±a https://infoscience.epfl.ch/record/33110)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "<br>\n",
    "In this notebook, we use three datasets: Cancer, Breast Cancer Wisconsin (Diagnostic) BCWD, and GOLUB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO in this notebook\n",
    "You should provide your answers to the questions of this notebook in a report (Note that a short and concise report with the essential information is **much** better than a long one that tells nothing...). Just indicate clearly the number of the question and give the respective answer. If you need plots to confirm your observations, include them also. At the end, send the notebook<b>-s</b> in annex to your report.\n",
    "<br>\n",
    "Sometimes you will need to select (decide on) some values as a way to perform filters that reduce the number of models (and save the bests).\n",
    "<br>\n",
    "At every question you will see which parts that question applies to: <b>0 for Cancer, 1 for BCWD, or 2 for GOLUB </b>.  You need to answer the questions according to the relevant dataset (see more instructions below).\n",
    "<br>\n",
    "To perform this lab you need to create three copies of the notebook (one for each dataset) and put your results in different paths and files (you will see along the lab).\n",
    "<br>\n",
    "<b>Some experiences take time (up to several hours), consider that in order to don't do your lab at the last minute (all the experiments are potentially different as TREFLE isn't a deterministic algorithm).\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to submit your lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export all the notebooks in HTML format (in the case your lab could not be reproduced for any reason) + zip your whole lab folder without the datasets. If your lab requires additional dependencies, please add a INSTRUCTIONS.md file in your folder with the instructions to install them. Don't forget to add the additional dependencies at the end of your requirements.txt file (or do a pip freeze > requirements_personal.txt command).\n",
    "<br>\n",
    "You must write a short report (as mentioned above) with three sections (one for each dataset) and respond to all the questions in each section. Include the report (<b>PDF and only PDF</b>) on the zip. The organization of the report must follow the structure below:\n",
    "<ul>\n",
    "    <li>Dataset Cancer</li>\n",
    "    <ul>\n",
    "        <li>Question 1 </li>\n",
    "        <li>Question ... </li>\n",
    "    </ul>\n",
    "    <li>Dataset BCWD</li>\n",
    "    <ul>\n",
    "        <li>Question 1 </li>\n",
    "        <li>Question ... </li>\n",
    "    </ul>\n",
    "    <li>Dataset GOLUB</li>\n",
    "    <ul>\n",
    "        <li>Question 1 </li>\n",
    "        <li>Question ... </li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install python=3.6 --yes\n",
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from itertools import tee\n",
    "\n",
    "\n",
    "\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from trefle.fitness_functions.output_thresholder import round_to_cls\n",
    "from trefle.trefle_classifier import TrefleClassifier\n",
    "\n",
    "from trefle_engine import TrefleFIS\n",
    "\n",
    "\n",
    "#Personal libraries\n",
    "\n",
    "import libraries.measures_calculation\n",
    "import libraries.trefle_project\n",
    "import libraries.interpretability_methods\n",
    "import libraries.interpretability_plots\n",
    "import libraries.results_plot\n",
    "from libraries.model_var import ModelVar\n",
    "from libraries.model_train_cv import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset\n",
    "<br>\n",
    "The first step of the ML process is to split our dataset into training and test parts (subsets). <br> \n",
    "<ul>\n",
    "    <li>You need to indicate the path of your original dataset</li>\n",
    "    <li>You need to indicate the path where you want to save the training part</li>\n",
    "    <li>You need to indicate the path where you want to save the test part</li>\n",
    "</ul>\n",
    "<br>When a plot is \"open\" you need to \"shut it down\" in oder to plot the others (button on the upper corner right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 Cancer\n",
    "\n",
    "#Read Dataset\n",
    "#Indicate the path of the original DS HERE:\n",
    "#--------------------\n",
    "csv_path_file_name = './Datasets/Cancer/CancerDiag2_headers.csv'\n",
    "#--------------------\n",
    "\n",
    "data_load = pd.read_csv(csv_path_file_name, sep = ';')\n",
    "\n",
    "#Before continue, please remove the variables that you don't want to use along this lab.\n",
    "\n",
    "#Use to drop columns\n",
    "#data_load.drop(['p1', 'p13'], axis=1)\n",
    "\n",
    "X = data_load.iloc[:, 1:-1]\n",
    "y = data_load.iloc[:,-1]\n",
    "\n",
    "#Split it into train test DS\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, stratify=y, random_state=42, test_size=0.33)\n",
    "\n",
    "plt.hist(y_train, bins='auto', label='Train')\n",
    "\n",
    "plt.hist(y_test, bins='auto', label='Test')\n",
    "plt.title(\"Train test Split\")\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Quantity')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Save separetly in training and test\n",
    "#It is important to save the training and test sets (we will use the test in the second part)\n",
    "y_train_modify = np.reshape(y_train, (-1, 1))\n",
    "train_dataset = np.append(X_train, y_train_modify, axis=1)\n",
    "\n",
    "y_test_modify = np.reshape(y_test, (-1, 1))\n",
    "test_dataset = np.append(X_test, y_test_modify, axis=1)\n",
    "\n",
    "#This indicates to numpy how to format the output (you can create a function for a larger number of variables...)\n",
    "format_values = ' %1.3f %1.3f %1.3f %1.3f %1.3f %1.3f %1.3f %1.3f %1.3f %1.3f %1.3f %1.3f %1.3f %1.3f %1.3f %i'\n",
    "\n",
    "\n",
    "#Indicate the path where you want to save the training and test part DS HERE:\n",
    "#--------------------\n",
    "path_train_csv = './Datasets/Cancer/CancerDiag2_headers_train.csv'\n",
    "path_test_csv = './Datasets/Cancer/CancerDiag2_headers_test.csv'\n",
    "#--------------------\n",
    "\n",
    "np.savetxt(path_train_csv, train_dataset, delimiter=\",\", fmt =format_values)\n",
    "np.savetxt(path_test_csv, test_dataset, delimiter=\",\", fmt = format_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 WDBC\n",
    "\n",
    "from sklearn import preprocessing\n",
    "#Read Dataset\n",
    "#Indicate the path of the original DS HERE:\n",
    "#--------------------\n",
    "csv_path_file_name = './Datasets/WDBC/data_WDBC.csv'\n",
    "#--------------------\n",
    "\n",
    "data_load = pd.read_csv(csv_path_file_name, sep=\",\")\n",
    "data_load = data_load.dropna(axis=1) # remove last colunm which only contains NaN values\n",
    "data_load.head()\n",
    "\n",
    "X = data_load.drop(['id', 'diagnosis'], axis=1).values\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(data_load[\"diagnosis\"].values)\n",
    "y = le.transform(data_load[\"diagnosis\"].values)\n",
    "\n",
    "#Split it into train test DS\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42, test_size=0.33)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "plt.hist(y_train, bins='auto', label='Train')\n",
    "\n",
    "plt.hist(y_test, bins='auto', label='Test')\n",
    "plt.title(\"Train test Split\")\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Quantity')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Save separetly in training and test\n",
    "#It is important to save the training and test sets (we will use the test in the second part)\n",
    "y_train_modify = np.reshape(y_train, (-1, 1))\n",
    "train_dataset = np.append(X_train, y_train_modify, axis=1)\n",
    "\n",
    "y_test_modify = np.reshape(y_test, (-1, 1))\n",
    "test_dataset = np.append(X_test, y_test_modify, axis=1)\n",
    "\n",
    "#This indicates to numpy how to format the output (you can create a function for a larger number of variables...)\n",
    "\n",
    "format_values=''\n",
    "for i in range(0,X_train.shape[1]):\n",
    "    format_values+=\" %1.3f\"\n",
    "format_values += \" %i\"\n",
    "\n",
    "#Indicate the path where you want to save the training and test part DS HERE:\n",
    "#--------------------\n",
    "path_train_csv = './Datasets/WDBC/data_WDBC_train.csv'\n",
    "path_test_csv = './Datasets/WDBC/data_WDBC_test.csv'\n",
    "#--------------------\n",
    "\n",
    "np.savetxt(path_train_csv, train_dataset, delimiter=\",\", fmt =format_values)\n",
    "np.savetxt(path_test_csv, test_dataset, delimiter=\",\", fmt = format_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 GOLUB\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import glob\n",
    "#Read Dataset\n",
    "#Indicate the path of the original DS HERE:\n",
    "#--------------------\n",
    "csv_path_file_name_train = \"./Datasets/golub/train/*.csv\"\n",
    "csv_path_file_name_test = \"./Datasets/golub/test/*.csv\"\n",
    "\n",
    "train_filenames = glob.glob(csv_path_file_name_train)\n",
    "test_filenames = glob.glob(csv_path_file_name_test)\n",
    "#--------------------\n",
    "le_classes = preprocessing.LabelEncoder()\n",
    "le_classes.fit([\"AML\", \"ALL\"])\n",
    "\n",
    "def parse_dataset(filenames, le_classes):\n",
    "    df = pd.read_csv(filenames[0], sep=\"\\t\", usecols=[\"ID_REF\", \"VALUE\"]).transpose().drop(\"ID_REF\", axis=0)\n",
    "    df.index = [filenames[0].split(\"/\")[-1]]\n",
    "\n",
    "    for i in range(1, len(filenames)):\n",
    "        df2 = pd.read_csv(filenames[i], sep=\"\\t\", usecols=[\"ID_REF\", \"VALUE\"]).transpose().drop(\"ID_REF\", axis=0)\n",
    "        df2.index = [filenames[i].split(\"/\")[-1]]\n",
    "\n",
    "        df = pd.concat([df, df2])\n",
    "\n",
    "    X = df.values\n",
    "\n",
    "    y = [fname[-7:-4] for fname in df.index.values]\n",
    "    y = le_classes.transform(y)\n",
    "\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = parse_dataset(train_filenames, le_classes)\n",
    "X_test, y_test = parse_dataset(test_filenames, le_classes)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "X = np.append(X_train, X_test, axis=0)\n",
    "y = np.append(y_train, y_test, axis=0)\n",
    "\n",
    "#Split it into train test DS\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42, test_size=0.33)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "plt.hist(y_train, bins='auto', label='Train')\n",
    "\n",
    "plt.hist(y_test, bins='auto', label='Test')\n",
    "plt.title(\"Train test Split\")\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Quantity')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Save separetly in training and test\n",
    "#It is important to save the training and test sets (we will use the test in the second part)\n",
    "y_train_modify = np.reshape(y_train, (-1, 1))\n",
    "train_dataset = np.append(X_train, y_train_modify, axis=1)\n",
    "\n",
    "y_test_modify = np.reshape(y_test, (-1, 1))\n",
    "test_dataset = np.append(X_test, y_test_modify, axis=1)\n",
    "\n",
    "#This indicates to numpy how to format the output (you can create a function for a larger number of variables...)\n",
    "\n",
    "format_values=''\n",
    "for i in range(0,X_train.shape[1]):\n",
    "    format_values+=\" %1.3f\"\n",
    "format_values += \" %i\"\n",
    "\n",
    "#Indicate the path where you want to save the training and test part DS HERE:\n",
    "#--------------------\n",
    "path_train_csv = './Datasets/WDBC/data_WDBC_train.csv'\n",
    "path_test_csv = './Datasets/WDBC/data_WDBC_test.csv'\n",
    "#--------------------\n",
    "\n",
    "np.savetxt(path_train_csv, train_dataset, delimiter=\",\", fmt =format_values)\n",
    "np.savetxt(path_test_csv, test_dataset, delimiter=\",\", fmt = format_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"background-color:red;color:white\">Question 1 (part 0, 1, and 2)</b>: Comment the plot above (include it into your report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trefle Classifier\n",
    "<br> In the code below you have a description of the (fuzzy logic-based) classifier that we use along this labo, the theory is provided in the slides of the cours. <br>\n",
    "Don't forget to change, if necessary, the number of generations (iterations) of your algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize our classsifier TREFLE\n",
    "clf = TrefleClassifier(\n",
    "    n_rules=4,\n",
    "    n_classes_per_cons=[2],  # there is only 1 consequent with 2 classes\n",
    "    n_labels_per_mf=3,  # use 3 labels LOW, MEDIUM, HIGH\n",
    "    default_cons=[0],  # default rule yield the class 0\n",
    "    n_max_vars_per_rule=3,  # WBCD dataset has 30 variables, here we force\n",
    "    # to use a maximum of 3 variables per rule\n",
    "    # to have a better interpretability\n",
    "    # In total we can have up to 3*4=12 different variables\n",
    "    # for a fuzzy system\n",
    "    n_generations=100,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and predicting with Trefle\n",
    "<br> Below you have just a simple example of how to:<br>\n",
    "<ul>\n",
    "    <li>train a model and make a prediction with it</li>\n",
    "    <li>save the model in a file</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a train\n",
    "y_sklearn = np.reshape(y_train, (-1, 1))\n",
    "\n",
    "clf.fit(X_train, y_sklearn)\n",
    "# Make predictions\n",
    "y_pred = clf.predict_classes(X_test)\n",
    "\n",
    "clf.print_best_fuzzy_system()\n",
    "\n",
    "# Evaluate accuracy\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print(\"Score on test set: {:.3f}\".format(score))\n",
    "\n",
    "tff = clf.get_best_fuzzy_system_as_tff()\n",
    "\n",
    "# Export: save the fuzzy model to disk\n",
    "with open(\"my_saved_model_trefle.tff\", mode=\"w\") as f:\n",
    "    f.write(tff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch the Trefle experiments (or modeling runs)\n",
    "In this labo we perform a k-fold cross-validation, so you must indicate how many folds do you want (by default 10). We could perform an exhaustive parameter search, but for this labo we will only search for parameters related with the size (complexity) of the model: i.e., number of rules and variables per rule.\n",
    "You must indicate where do you want to save all the models obtained. <br>\n",
    "\n",
    "**Important:** You must choose and justify the range of values you will explore for:\n",
    "<ul>\n",
    "    <li>the number of rules</li>\n",
    "    <li>the maximum number of variables per rule </li>\n",
    "<ul>\n",
    "The code must be adapted according to your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitness function\n",
    "The fitness function (or performance function) allows you to choose which performance metrics you want to improve (maximise), in our case the sensitivity and specificity (You can see the slides for more details).\n",
    "<br>\n",
    "IMPORTANT: analyze the comments in the code, perform the modifications that are necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "##############fitness function (No change required)\n",
    "def fit (y_true, y_pred):\n",
    "    \n",
    "    y_pred_bin = round_to_cls(y_pred, n_classes=2)\n",
    "    tn, fp, fn, tp = libraries.trefle_project.getConfusionMatrixValues(y_true, y_pred_bin)\n",
    "    \n",
    "    \n",
    "    sensitivity = libraries.measures_calculation.calculateSensitivity(tn, fp, fn, tp)\n",
    "    specificity = libraries.measures_calculation.calculateSpecificity(tn, fp, fn, tp)\n",
    "    rmse = mean_squared_error(y_true, y_pred)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    score = 0.2 * math.pow(2, -rmse) + 0.4*sensitivity + 0.4 * specificity\n",
    "    return score\n",
    "\n",
    "clf.fitness_function=fit\n",
    "###############\n",
    "\n",
    "#Perform Cross-validation\n",
    "k_fold_number = 10\n",
    "cv_kf = KFold(n_splits=k_fold_number, random_state=42, shuffle=True)\n",
    "array_index_train_test = cv_kf.split(X_train)\n",
    "array_index_train_test, array_index_train_test_copy = tee(array_index_train_test)\n",
    "\n",
    "\n",
    "###############\n",
    "\n",
    "#--------------------\n",
    "#Path where you want to save your models (you need to create the directory before starting the algorithm)\n",
    "path_save_results_directory = 'results_models_lab_2_p0/'\n",
    "#file name that will contain the results for each model created (for each fold)\n",
    "file_results_dv = 'values__lab0_p0.csv'\n",
    "#Name of the experience, this name will appear on the models files\n",
    "experience_value_name = 'exps_lab0_p0'\n",
    "#--------------------\n",
    "\n",
    "model_train_obj = ModelTrain(array_index_train_test = array_index_train_test,\n",
    "                             X_train = X_train,\n",
    "                             y_train = y_train, \n",
    "                             number_rule = 0, var_per_rule = 0, \n",
    "                             classifier_trefle = clf, \n",
    "                             path_save_results = path_save_results_directory,\n",
    "                            path_save_results_values=file_results_dv,\n",
    "                            experience_name = experience_value_name)\n",
    "\n",
    "#Here we can choose the values/ranges for the number of rules and maximum variables per rule \n",
    "#that we want to test along our experience \n",
    "#('here you must change values and explain your choice, in the report')\n",
    "rules_number_vec = [3, 2]\n",
    "var_per_rule_vec = [3, 2]\n",
    "\n",
    "for variation_a in rules_number_vec:\n",
    "    for variation_b in var_per_rule_vec:\n",
    "        model_train_obj.number_rule = variation_a\n",
    "        model_train_obj.var_per_rule = variation_b\n",
    "        model_train_obj.execute_cv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"background-color:red;color:white\">Question 2 (part 0)</b>: Explain what role play, in the specific model, the number of rules and the number of variables per rule\n",
    "<br>\n",
    "<b style=\"background-color:red;color:white\">Question 3 (part 0)</b>: If one uses 5 rules with 6 variables per rule in a dataset with 100 features, how many features can be used at maximum?\n",
    "<br>\n",
    "<b style=\"background-color:red;color:white\">Question 4 (parts 0, 1, and 2)</b>: Explain your choice of the number of rules and variables per rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List result files\n",
    "When all the modeling experiments are performed, we calculate the average of the scores for each configuration according to the number of folds for several metrics/measurements (accuracy, f1-score, sensitivity, and specificity). \n",
    "<br>Don't forget to change the file where you have the results for the models (you changed previously \"file_results_dv\")\n",
    "<br>For curiousity sake, you may implement other metrics in the \"measures_calculation\" class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "\n",
    "#play with the results of the differents executions\n",
    "data = pd.read_csv(\"values__lab0_p0.csv\") \n",
    "\n",
    "param_a_designation = 'nb of rules'\n",
    "param_b_designation = 'nb of var per rule'\n",
    "\n",
    "vec_measures = ['acc', 'f1', 'sen', 'spe']\n",
    "\n",
    "\n",
    "\n",
    "test_data = data.iloc[:,0:2]\n",
    "\n",
    "\n",
    "data_frame_treated = libraries.trefle_project.treatmentResultsValues(data, param_a_designation, param_b_designation, vec_measures)\n",
    "data_frame_treated.columns = ['N rule', 'N var per rule', 'acc', 'f1', 'sen', 'spe']\n",
    "display(data_frame_treated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize with 3D graphs\n",
    "Below you can visualize the performance of your models according to the explored paramaters: number of rules and variables per rule. You may change the code so as to make plots for different metrics (Acc, F1, Sen and Spe). As mentioned, you may also add new metrics by creating the method in the 'measures_calculation' class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot 3D\n",
    "%matplotlib notebook\n",
    "\n",
    "\n",
    "\n",
    "X = data_frame_treated['N rule']\n",
    "Y = data_frame_treated['N var per rule']\n",
    "Z = data_frame_treated['acc']\n",
    "\n",
    "y_axis_values = range(math.floor(min(Y)), math.ceil(max(Y))+1)\n",
    "x_axis_values = range(math.floor(min(X)), math.ceil(max(X))+1)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "surf = ax.plot_trisurf(X, Y, Z,  cmap=cm.YlGnBu, linewidth=0, antialiased=False)\n",
    "\n",
    "#ax.set_zlim(-1.01, 1.01)\n",
    "ax.set_xticks(x_axis_values, minor=False)\n",
    "ax.set_yticks(y_axis_values, minor=False)\n",
    "\n",
    "ax.set_xlabel('$Number of rules$')\n",
    "ax.set_ylabel('$Number of var per rule$')\n",
    "\n",
    "\n",
    "ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "\n",
    "\n",
    "\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "plt.title('Original Code')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"background-color:red;color:white\">Question 5 (parts 0,1, and 2)</b> In your opinion, which values/ranges of both parameters: number of rules and vars per rule, should you choose to obtain the bests models? (comment briefly on the plot and include it into to report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refine the parameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we observed the plot we can refine the search for parameter values. As for the previous experiment it is necessary to:\n",
    "<ul>\n",
    "    <li>define new values/ranges for the number of rules </li>\n",
    "    <li>define new values/ranges number of variable per rule </li>\n",
    "    <li>change the path name where you want to save the new models </li>\n",
    "    <li>change the name of the file that will contain the number of experiments</li>\n",
    "     \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Here we can choose wich values for the number of rules and maximum variable per \n",
    "#rule we want to test along our experience \n",
    "#('here you need to change and explain your choice, on the repport')\n",
    "#--------------------\n",
    "rules_number_vec = [4,5]\n",
    "var_per_rule_vec = [4,5]\n",
    "#--------------------\n",
    "\n",
    "#Change the path directory where you want to save the new results\n",
    "#--------------------\n",
    "model_train_obj.path_save_results = 'results_models_lab_2_p0_tuning/'\n",
    "model_train_obj.path_save_results_values='values_lab0_p0_tuning.csv'\n",
    "#--------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#--------------------\n",
    "model_train_obj = ModelTrain(array_index_train_test = array_index_train_test_copy,\n",
    "                             X_train = X_train,\n",
    "                             y_train = y_train, \n",
    "                             number_rule = 0, var_per_rule = 0, \n",
    "                             classifier_trefle = clf, \n",
    "                             path_save_results = 'results_models_lab_2_p0_tuning/',\n",
    "                            path_save_results_values='values_lab0_p0_tuning.csv',\n",
    "                             experience_name = 'second_exp')\n",
    "#--------------------\n",
    "\n",
    "for variation_a in rules_number_vec:\n",
    "    for variation_b in var_per_rule_vec:\n",
    "        model_train_obj.number_rule = variation_a\n",
    "        model_train_obj.var_per_rule = variation_b\n",
    "        model_train_obj.execute_cv()\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"background-color:red;color:white\">Question 6 (part 0, 1, and 2)</b>: Explain your choice for the refined search of number of rules and variables per rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read all csv\n",
    "#--------------------\n",
    "dataframe_results = pd.read_csv('values__lab0_p0.csv')\n",
    "dataframe_results_b = pd.read_csv('values_lab0_p0_tuning.csv')\n",
    "#--------------------\n",
    "\n",
    "dataframe_results_c = dataframe_results_b.append(dataframe_results)\n",
    "\n",
    "#dataframe_results_c = pd.read_csv('values_w.csv')\n",
    "dataframe_results_c.columns = ['N rule', 'N var per rule','CV number', 'tn', 'fp', 'fn', 'tp', 'file_name']\n",
    "dataframe_results_c.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have tested all the configurations, we have obtained a (large) number of models exhibiting diverse performance metrics. The goal being to select the best models, a first selection is performed by applying a filter based on the diagnostic performance, thus reducing the number of models. Below you can see a scatter plot of all the models you obtained according to their sensitivity and specificity (as obtained on the validation subsets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "\n",
    "#Plot all values\n",
    "#don't forget to turn off the others plots\n",
    "vec_values_sen_spe_models = libraries.interpretability_methods.getSenSpeValuesByScores(dataframe_results_c)\n",
    "#vec_values_sen_spe_models = libraries.interpretability_methods.getSenSpeValuesByScores(data_frame_treated)\n",
    "\n",
    "plt.scatter(vec_values_sen_spe_models['Sensitivity'],vec_values_sen_spe_models['Specificity'],s=10, marker='o')\n",
    "\n",
    "plt.title('Threshold sen/spe')\n",
    "plt.xlabel('Sensitivity')\n",
    "plt.ylabel('Specificity')\n",
    "plt.savefig('ScatterPlot.png')\n",
    "\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('In total you have {0} models'.format(len(vec_values_sen_spe_models)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First selection filter: based on sen/spe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having analysed the above performance overview of your models, you can apply a filter based on  sensitivity and specificity. In this way, only those models exhibiting better performance than some specified threshold will be selected for the next step.\n",
    "The plot below shows the effect of the combined thresholds on the number of models remaining after the filter is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#that save x models\n",
    "%matplotlib notebook\n",
    "\n",
    "results_qty_models = libraries.interpretability_methods.plotSenSpeQtyModelsByThreshold(vec_values_sen_spe_models)\n",
    "\n",
    "\n",
    "X = results_qty_models['sensitivity']\n",
    "Y = results_qty_models['specificity']\n",
    "Z = results_qty_models['qty_models']\n",
    "\n",
    "\n",
    "max_quantity = results_qty_models.loc[results_qty_models['qty_models'].idxmax()]\n",
    "max_quantity = int(max_quantity['qty_models'])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "surf = ax.plot_trisurf(X, Y, Z.values,  cmap=cm.YlGnBu, linewidth=0, antialiased=False)\n",
    "\n",
    "ax.set_zticks(Z)\n",
    "\n",
    "ax.set_xlabel('$Sensitivity$')\n",
    "ax.set_ylabel('$Specificity$')\n",
    "\n",
    "\n",
    "\n",
    "ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "ax.zaxis.set_major_formatter(FormatStrFormatter('%.0f'))\n",
    "\n",
    "\n",
    "\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "plt.title('Sen/Spe threshold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the base of this plot, you should decide on threshold values for both, specificity and sensitivity and apply them. The resulting subset of selected models is shown in the scatterplot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "#Put a limit in sen/spe\n",
    "#Here you put the threshold for the sensitivity and specificity\n",
    "#Don't forget to shave the plot and comment into your repport\n",
    "#--------------------\n",
    "value_sensitivity = 0.6\n",
    "value_specificity = 0.6\n",
    "#--------------------\n",
    "\n",
    "\n",
    "#We apply them\n",
    "\n",
    "vec_values_sen_spe_models_filtered = libraries.interpretability_methods.filterDataframeBySenSpeLimit(value_sensitivity, value_specificity, vec_values_sen_spe_models)\n",
    "vec_values_sen_spe_models_filtered_invert = libraries.interpretability_methods.filterDataframeBySenSpeLimitContrary(value_sensitivity, value_specificity, vec_values_sen_spe_models)\n",
    "\n",
    "\n",
    "figure = libraries.interpretability_plots.plotDataFrameValuesFiltered(value_sensitivity, value_specificity,vec_values_sen_spe_models_filtered, vec_values_sen_spe_models_filtered_invert)\n",
    "print('In total you have {0} models'.format(len(vec_values_sen_spe_models_filtered)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"background-color:red;color:white\">Question 7 (part 0, 1, and 2)</b>: Explain your choice of the threshold values for the sensitivity and specificity. (Save both plots into your repports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second selection: frequency-based filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a second model-selection filter is applied based on the \"importance\" of the features. Such feature importance is represented in this context by their relative presence (i.e. their frequency) among the models. \n",
    "\n",
    "#### Frequency of the variables\n",
    "The figure below shows the frequency of the variables among all the remaining models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Don't forget to change the names of the files...\n",
    "\n",
    "\n",
    "dataframe_first = pd.read_csv(\"values__lab0_p0.csv\") \n",
    "dataframe_weight = pd.read_csv(\"values_lab0_p0_tuning.csv\")\n",
    "\n",
    "\n",
    "frames = [dataframe_first, dataframe_weight]\n",
    "result = pd.concat(frames)\n",
    "\n",
    "result = vec_values_sen_spe_models_filtered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Don't forget to put all your models in a single path... </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "  \n",
    "\n",
    "list_models_path_complete = []\n",
    "for index, row in vec_values_sen_spe_models_filtered.iterrows():\n",
    "    model_path_complete = \"results_models_lab_2_p0_all/\" + str(row['file_name'])\n",
    "    list_models_path_complete.append(model_path_complete)\n",
    "    \n",
    "#Perform the counting\n",
    "list_models_vars = libraries.interpretability_methods.transformModelsToModelVarObj(list_models_path_complete)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot histogram before cut\n",
    "\n",
    "dict_values_resultant = libraries.interpretability_methods.countVarFreq(list_models_vars)\n",
    "\n",
    "\n",
    "#indication of the number of models and variables\n",
    "qty_models = len(list_models_vars)\n",
    "qty_variables = len(dict_values_resultant)\n",
    "print(\"You have {0} models and {1} variables\".format(qty_models, qty_variables))\n",
    "\n",
    "#Plot the new histogram\n",
    "libraries.interpretability_plots.plotHistogramFreqVar(dict_values_resultant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing a frequency threshold\n",
    "Filtering features by frequency will result in a reduction of both the number of features and the number of models, as models with eliminated variables are also eliminated. \n",
    "\n",
    "The plot below represents the number of models and variables that should remain after the filter is applied in function of the frequency threshold. It helps you to decide on which threshold to use for the filter.\n",
    "\n",
    "(Note that the frequency of a feature is calculated as the number of <b>different models</b> where it appears irrespective of the number of rules containing it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_values = libraries.interpretability_methods.countVarFreq(list_models_vars)\n",
    "\n",
    "\n",
    "matrix_results = libraries.interpretability_methods.createPlotQtyVarPerModelByMinimumFreq(dict_values,list_models_vars)\n",
    "display(matrix_results)\n",
    "\n",
    "\n",
    "libraries.interpretability_plots.plotFreqVarPerFreqMinimum(matrix_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on the plot above, select the minimum frequency (threshold) for the variables on your models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"background-color:red;color:white\">Question 8 (part 0, 1, and 2)</b>: Explain your choice of the threshold. (Save both plots into your report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to indicate the name of the file where you want to save the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the frequency value\n",
    "\n",
    "#Create a copy of the list that contains the model_var objects\n",
    "list_models_vars_cpopy = list_models_vars.copy()\n",
    "#select the minimum frequency\n",
    "#--------------------\n",
    "nb_min_var = 3\n",
    "#--------------------\n",
    "\n",
    "#Perform the frequency\n",
    "list_model_var_resultant = libraries.interpretability_methods.reduceQtyVars(nb_min_var, dict_values,list_models_vars_cpopy)\n",
    "dict_values_resultant = libraries.interpretability_methods.countVarFreq(list_model_var_resultant)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#indication of the number of models and variables\n",
    "qty_models = len(list_model_var_resultant)\n",
    "qty_variables = len(dict_values_resultant)\n",
    "print(\"You have now {0} models and {1} variables\".format(qty_models, qty_variables))\n",
    "\n",
    "#Plot the new histogram\n",
    "libraries.interpretability_plots.plotHistogramFreqVar(dict_values_resultant)\n",
    "#Show the frequency table\n",
    "dict_Values_ordered = libraries.interpretability_methods.sort_reverse_dictionary_by_values(dict_values_resultant)\n",
    "datafram_var_freq = pd.DataFrame(list(dict_Values_ordered.items()),columns=['Variable name','Frequence'])\n",
    "display(datafram_var_freq)\n",
    "\n",
    "\n",
    "#Perform the list of the models\n",
    "#--------------------\n",
    "file_name = 'models_selected.csv'\n",
    "#--------------------\n",
    "list_models_names=[model_var.model_path for model_var in list_model_var_resultant]\n",
    "dataframe_names_files = pd.DataFrame(list_models_names)\n",
    "dataframe_names_files.to_csv(file_name, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of the selected models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have selected the best models, they are saved on the file \"models_selected.CSV\" (Or other file if you change the name...)\n",
    "You may then load these models and use them to compute their predictions for the observations in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "# Import from file\n",
    "#--------------------\n",
    "fis = TrefleFIS.from_tff_file(\"results_models_lab_2_p0_all/second_exp_conf_A_CV_0_rule_4_var_per_rule_4.ftt\")\n",
    "#--------------------\n",
    "\n",
    "\n",
    "# In the future, it could possible to call clf.predict_classes() directly\n",
    "# see issue #1\n",
    "y_pred_test = fis.predict(X_test)\n",
    "\n",
    "results_list_predictions = np.squeeze(np.asarray(y_pred_test))\n",
    "\n",
    "\n",
    "#libraries.results_plot.plotCMByTreflePredictions(y_test, results_list_predictions)\n",
    "#Convert your results into binary values\n",
    "results = []\n",
    "for element in y_pred_test:\n",
    "    if element > 0.5:\n",
    "        results.append(1)\n",
    "    else:\n",
    "        results.append(0)\n",
    "\n",
    "from libraries.ConfusionMatrix import ConfusionMatrix\n",
    "cm = confusion_matrix(y_test, results)\n",
    "n_classes = len(np.unique(y))\n",
    "ConfusionMatrix.plot(cm, classes=range(n_classes), title=\"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above is only an example of how to load models and test their performance in the test set. (Remember that the test set is the one who has not been used during the previous training/selectionn steps.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><span style=\"background-color:red;color:white\">Question 9 (parts 1 and 2)</span></b>: Among the final models, select three of them as follows: the smallest one (in terms of rules and variables), the best one (in terms of perfromance), and one in the \"middle\" that you consider as being a good trade off between size and performance. With them:\n",
    "<ul>\n",
    "    <li>Apply them to the test set and analyze the rfesults you obtained</li>\n",
    "    <li>Analyze them in terms of size, rules, vars per rules and other characteristics that you think are relevant</li>\n",
    "    <li>As far as possible, analyze their rules and try to \"explain\" their predictions.\n",
    "</ul>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><span style=\"background-color:red;color:white\">Question 10 (parts 1 and 2)</span></b>: Compare the features (variables) that were selected by Trefle (This algorithm, you know?) with those obtained in your previous laboratory on feature selection. Conclude.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mse-mldb-lab4]",
   "language": "python",
   "name": "conda-env-mse-mldb-lab4-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
